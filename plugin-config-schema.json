{
  "type": "object",
  "required": ["userAgent"],
  "properties": {
    "userAgent": {
      "type": "string",
      "title": "Robot identifier",
      "description": "User-Agent header communicated to the web servers and used to determine the relevant rules from robots.txt files. Example: \"data-fair-web-scraper-koumoul\"",
      "default": "data-fair-web-scraper"
    },
    "defaultCrawlDelay": {
      "type": "integer",
      "title": "Number of seconds to wait between each page download",
      "description": "Can be overridden by robots.txt Crawl-delay rules",
      "default": 1
    }
  }
}
